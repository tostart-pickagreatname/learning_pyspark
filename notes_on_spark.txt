Spark notes:

THIS FILE CONTAINS A LOT OF SPORADIC NOTES AND SOME DUPLICATION FROM https://spark.apache.org/docs/latest/programming-guide.html

Spark context: this is how the application knows how to connect to spark. In the PySpark shell, a special interpreter-aware SparkContext is already created for me in a variable called `sc`.

You may also add dependencies (spark packages, for example), to your session session via the —packages argument. Additionally, you may use the —repositories argument for additional dependencies.

—master local[4] <- causes pyspark to run on exactly 4 cores

The run command looks like: ./bin/pyspark --master local[4] --py-files code.py
which leverages the spark-submit script.

RDD:

fault-tolerant collection which can be operated on in parallel.
Created by:
  1. SparkContext’s parallelize method used on an existing utterable or collection
  2. referencing a dataset in an external storage system (HDFS, shared filesystem, Hadoop InputFormat, HBase)

Supports transformations (produce new data set) and actions (return a value).
Can persist RDD in memory using cache method to keep the elments on the cluster for faster access the next time it's queried. Also support for persisting RDDs on disk, or replicating across multiple nodes.

Passing functions to SparkConf

Longer functions are often wrapped in a `if __name__ == "__main__"` clause. These scripts are then run later, and wrapping them this way prevents them from execution at import time if that occurs.

Local vs. Cluster Modes
Execution is split up into a series of tasks. Each task executor is then supplied with a serialized object containing the variables required to execute the task, this serialized object is called the "closure".
What is important is that this closure is referenced only locally, creating new instance variables when it does. So it is not safe to assume that a loop, for example, will update the value of a variable initialized outside of the content of that loop. It may update a variable with the same name and starting value, but that variable will be local to the memory of the excutor, and therefore it won't be modifying the original variable.
Accumulators are a way for Spark to deal with this problem. "Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster."
Constructors like loops or locally defined methods should not be used to mutate some global state. Accumulators are REQUIRED for these things to work in global mode (whereas anything may work in local, since it isn't sending closures around).
Things like `rdd.foreach(println)` or `rdd.map(println)` will, in global mode, sending strings to the `stdout` of the executor, so the driver's interface won't show these. `.collect()` can be used to bring the RDD to the driver node, which would then allow it to be displayed. One can also use `.take(10)` to grab just ten of the items locally to print, etc.
